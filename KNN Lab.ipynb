{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVL7_bgmIAPR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# K-Nearest Neighbor Lab\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6ZbYjZZZ_yLV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import add\n",
    "from scipy.io import arff\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCcEPx5VIORj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Implement the k-nearest neighbor (KNN) algorithm\n",
    "\n",
    "### Code requirements\n",
    "- Use Euclidean distance to decide closest neighbors\n",
    "- Implement both the regular (classifcation) version and the regression version\n",
    "- Include optional distance weighting for both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def load_data(url: str):\n",
    "    ftp_stream = urllib.request.urlopen(url)\n",
    "    data, meta = arff.loadarff(io.StringIO(ftp_stream.read().decode('utf-8')))\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    return data_frame\n",
    "\n",
    "# Gets the euclidean distance between new point and all X rows for continuous data\n",
    "def real_dist(X, newPoint, ax=1):\n",
    "    x = X - newPoint\n",
    "    s = (x.conj() * x).real\n",
    "    sqrtFunction = np.vectorize(sqrt)\n",
    "    return sqrtFunction(add.reduce(s, axis=ax, keepdims=False))\n",
    "\n",
    "def sqrt(value):\n",
    "    return math.sqrt(value)\n",
    "\n",
    "def get_inv_dist_squared(distances):\n",
    "    return 1.0 / (distances**2+0.0000001)\n",
    "\n",
    "\n",
    "# Gets the top k rows from a matrix\n",
    "def get_top_k(matrix, k):\n",
    "    if len(matrix.shape) == 1:\n",
    "        return matrix[:k]\n",
    "    else:\n",
    "        return matrix[:k, :]\n",
    "\n",
    "# Gets a list of unique values for a 1d array\n",
    "def get_unique(array):\n",
    "    return np.unique(array)\n",
    "\n",
    "# Gets the most frequent number from an array\n",
    "def get_mode(array):\n",
    "    vals, counts = np.unique(array, return_counts=True)\n",
    "    mode_value = np.argwhere(counts == np.max(counts))\n",
    "    return vals[mode_value].flatten()[0]\n",
    "\n",
    "# Aggregates a 2d matrix by the group column, summing the agg column\n",
    "def get_sum_aggregates(array, group_col_index, agg_col_index):\n",
    "    unique_vals = get_unique(array[:, group_col_index])\n",
    "    my_aggregation = np.array([[unique_val, array[array[:,group_col_index]==unique_val,agg_col_index].sum()] for unique_val in unique_vals], dtype='O')\n",
    "    my_sorted_aggregation = my_aggregation[(-my_aggregation[:,-1]).argsort()]\n",
    "    return my_sorted_aggregation\n",
    "\n",
    "def dot_col_vecs(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def get_num_elements(array):\n",
    "    product = 1\n",
    "    for elem in array.shape:\n",
    "        product *= elem\n",
    "    return product\n",
    "\n",
    "def decodeBytes(value):\n",
    "    if type(value) == 'bytes':\n",
    "        return value.decode()\n",
    "    return value\n",
    "\n",
    "def get_continuous_distance(inputFeaturePoint, predictionFeaturePoint):\n",
    "    if inputFeaturePoint == '?' or predictionFeaturePoint == '?':\n",
    "        return 1\n",
    "    return inputFeaturePoint - predictionFeaturePoint\n",
    "\n",
    "def get_nominal_distance(inputFeaturePoint, predictionFeaturePoint):\n",
    "    if inputFeaturePoint == '?' or predictionFeaturePoint == '?':\n",
    "        return 1\n",
    "    return int(inputFeaturePoint != predictionFeaturePoint)\n",
    "\n",
    "def normalize_data(array):\n",
    "    newarray = array.copy()\n",
    "    _, num_cols = array.shape\n",
    "    for i in range(0, num_cols):\n",
    "        curCol = array[:,i]\n",
    "        curColMax = np.max(curCol)\n",
    "        curColMin = np.min(curCol)\n",
    "        def normalize(value):\n",
    "            return (value - curColMin) / float(curColMax - curColMin)\n",
    "        normalizeVectorized = np.vectorize(normalize)\n",
    "        newarray[:,i] = normalizeVectorized(curCol)\n",
    "    return newarray\n",
    "\n",
    "def get_normalized_housing_data():\n",
    "    vectorizedDecoder = np.vectorize(decodeBytes)\n",
    "    training_data = vectorizedDecoder(np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_train.arff\")))\n",
    "    train_x = training_data[:,:-1]\n",
    "    train_y = training_data[:,-1]\n",
    "\n",
    "    testing_data = vectorizedDecoder(np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_test.arff\")))\n",
    "    test_x = testing_data[:,:-1]\n",
    "    test_y = testing_data[:,-1]\n",
    "\n",
    "    combined_data = np.concatenate((train_x, test_x))\n",
    "    combined_normalized = normalize_data(combined_data)\n",
    "    num_train_x_rows, _ = train_x.shape\n",
    "\n",
    "    normalized_train_x = combined_normalized[:num_train_x_rows, :]\n",
    "    normalized_test_x = combined_normalized[num_train_x_rows:, :]\n",
    "    return normalized_train_x, train_y, normalized_test_x, test_y\n",
    "\n",
    "def get_normalized_telescope_data():\n",
    "    vectorizedDecoder = np.vectorize(decodeBytes)\n",
    "\n",
    "    training_data = np.array(\n",
    "        load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff\"))\n",
    "    train_x = training_data[:, :-1]\n",
    "    train_y = vectorizedDecoder(training_data[:, -1])\n",
    "\n",
    "    testing_data = np.array(\n",
    "        load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff\"))\n",
    "    test_x = testing_data[:, :-1]\n",
    "    test_y = vectorizedDecoder(testing_data[:, -1])\n",
    "\n",
    "    combined_data = np.concatenate((train_x, test_x))\n",
    "    combined_normalized = normalize_data(combined_data)\n",
    "    num_train_x_rows, _ = train_x.shape\n",
    "\n",
    "    normalized_train_x = combined_normalized[:num_train_x_rows, :]\n",
    "    normalized_test_x = combined_normalized[num_train_x_rows:, :]\n",
    "    return normalized_train_x, train_y, normalized_test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_a2KSZ_7AN0G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class KNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, columntype=[], weight_type='inverse_distance', regression=False, hasNominal=False):  ## add parameters here\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            columntype for each column tells you if continues[real] or if nominal[categoritcal].\n",
    "            weight_type: inverse_distance voting or if non distance weighting. Options = [\"no_weight\",\"inverse_distance\"]\n",
    "        \"\"\"\n",
    "        self.columntype = columntype  # Note This won't be needed until part 5\n",
    "        self.weight_type = weight_type\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.regression = regression\n",
    "        self.has_nominal = hasNominal\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, k_array):\n",
    "        k_dictionary = {}\n",
    "        for row in X:\n",
    "            new_x, distances, new_y = self.get_distances(self.X, self.y, row)\n",
    "            for k in k_array:\n",
    "                new_x_copy = get_top_k(new_x, k)\n",
    "                new_y_copy = get_top_k(new_y, k)\n",
    "                distances_copy = get_top_k(distances, k)\n",
    "                if self.regression:\n",
    "                    if self.weight_type == 'inverse_distance':\n",
    "                        inv_distances = get_inv_dist_squared(distances_copy)\n",
    "                        numerator = dot_col_vecs(inv_distances, new_y_copy)\n",
    "                        denominator = inv_distances.sum()\n",
    "                        guess = float(numerator)/denominator\n",
    "                        self.add_k_guess(k_dictionary, guess, k)\n",
    "                    else:\n",
    "                        guess = new_y_copy.sum()/float(get_num_elements(new_y_copy))\n",
    "                        self.add_k_guess(k_dictionary, guess, k)\n",
    "                else:\n",
    "                    if self.weight_type == 'inverse_distance':\n",
    "                        inv_distances = get_inv_dist_squared(distances_copy)\n",
    "                        aggregates = get_sum_aggregates(np.append(inv_distances[..., None], new_y_copy[..., None], axis=1), 1, 0)\n",
    "                        guess = aggregates[0, 0]\n",
    "                        self.add_k_guess(k_dictionary, guess, k)\n",
    "                    else:\n",
    "                        guess = get_mode(new_y_copy)\n",
    "                        self.add_k_guess(k_dictionary, guess, k)\n",
    "\n",
    "        return k_dictionary\n",
    "\n",
    "    # Returns the Mean score given input data and labels\n",
    "    def score(self, X, y, k_array):\n",
    "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 2D numpy array with targets\n",
    "        Returns:\n",
    "            score : float\n",
    "                Mean accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "        k_dictionary = self.predict(X, k_array)\n",
    "        finalScores = []\n",
    "        for k in k_array:\n",
    "            results = k_dictionary[k]\n",
    "            if self.regression:\n",
    "                mse = ((y - results)**2).sum() / float(len(y))\n",
    "                finalScores.append(mse)\n",
    "            else:\n",
    "                correct = 0\n",
    "                for i in range(0, len(results)):\n",
    "                    if results[i] == y[i]:\n",
    "                        correct += 1\n",
    "                finalScores.append(float(correct) / len(results))\n",
    "        return finalScores\n",
    "\n",
    "    def get_prediction_by_count(self, y):\n",
    "        return get_mode(y)\n",
    "\n",
    "    def add_k_guess(self, k_dictionary, guess, k):\n",
    "        if k not in k_dictionary:\n",
    "            k_dictionary[k] = []\n",
    "        k_dictionary[k].append(guess)\n",
    "\n",
    "    # Returns the X and y values sorted by distance from the new point along with their corresponding distances\n",
    "    def get_distances(self, X, y, newPoint):\n",
    "        if not self.has_nominal:\n",
    "            distances = real_dist(X, newPoint)\n",
    "            augmented_with_y = np.append(X, np.column_stack([y]), axis=1)\n",
    "            augmented_with_dist = np.append(augmented_with_y, np.column_stack([distances]), axis=1)\n",
    "            _, num_cols = augmented_with_dist.shape\n",
    "            sorted_augmented = augmented_with_dist[augmented_with_dist[:, num_cols - 1].argsort()]\n",
    "            new_x = sorted_augmented[:, :-2]\n",
    "            new_y = sorted_augmented[:, -1]\n",
    "            dist = sorted_augmented[:, -2]\n",
    "            return new_x, new_y, dist\n",
    "        else:\n",
    "            all_distances = []\n",
    "            for i, row in enumerate(X):\n",
    "                row_distances = []\n",
    "                for j, col in enumerate(row):\n",
    "                    if self.columntype[j] == 'nominal':\n",
    "                        row_distances.append(get_nominal_distance(X[i, j], newPoint[j]))\n",
    "                    else:\n",
    "                        row_distances.append(get_continuous_distance(X[i, j], newPoint[j]))\n",
    "                row_distances = np.array(row_distances)\n",
    "                final_distance = np.sqrt(np.power(row_distances,2).sum())\n",
    "                all_distances.append(final_distance)\n",
    "            distances = np.array(all_distances)\n",
    "            augmented_with_y = np.append(X, np.column_stack([y]), axis=1)\n",
    "            augmented_with_dist = np.append(augmented_with_y, np.column_stack([distances]), axis=1)\n",
    "            _, num_cols = augmented_with_dist.shape\n",
    "            sorted_augmented = augmented_with_dist[augmented_with_dist[:, num_cols - 1].argsort()]\n",
    "            new_x = sorted_augmented[:, :-2]\n",
    "            new_y = sorted_augmented[:, -1]\n",
    "            dist = sorted_augmented[:, -2]\n",
    "            return new_x, new_y, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Debug and Evaluation\n",
    "\n",
    "Debug and Evaluate your model using the parameters below:\n",
    "- Use distance weighting\n",
    "- KNN = 3 (three nearest neighbors)\n",
    "- Don’t normalize the data\n",
    "- Use Euclidean Distance\n",
    "---\n",
    "\n",
    "### 1.1 (20%) Debug using this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_test.arff)\n",
    "\n",
    "Expected Results:\n",
    "- Not using inverse weighted distancing = roughly [68.29%]\n",
    "- Link to [debug solution](https://github.com/cs472ta/CS472/blob/master/debug_solutions/glass_no_inv_predictions.txt)\n",
    "- Using inverse weighted distancing = roughly [74.39%]\n",
    "- Link to [debug solution](https://github.com/cs472ta/CS472/blob/master/debug_solutions/glass_inv_predictions.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6829268292682927]\n",
      "[0.7439024390243902]\n"
     ]
    }
   ],
   "source": [
    "def do_debug():\n",
    "    vectorizedDecoder = np.vectorize(decodeBytes)\n",
    "\n",
    "    training_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_train.arff\"))\n",
    "    train_x = training_data[:,:-1]\n",
    "    train_y = vectorizedDecoder(training_data[:,-1])\n",
    "\n",
    "    testing_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_test.arff\"))\n",
    "    test_x = testing_data[:,:-1]\n",
    "    test_y = vectorizedDecoder(testing_data[:,-1])\n",
    "\n",
    "    knn = KNNClassifier(weight_type=\"\")\n",
    "    knn.fit(train_x, train_y)\n",
    "    print(knn.score(test_x, test_y, [3]))\n",
    "\n",
    "    knn2 = KNNClassifier()\n",
    "    knn2.fit(train_x, train_y)\n",
    "    print(knn2.score(test_x, test_y, [3]))\n",
    "do_debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 (20%) Evaluate\n",
    "\n",
    "We will evaluate your model based on its performance on the [diabetes](https://archive.ics.uci.edu/ml/datasets/Diabetes) problem.\n",
    "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_test.arff) and have your code print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8411458333333334]\n",
      "[0.890625]\n"
     ]
    }
   ],
   "source": [
    "def do_eval():\n",
    "    vectorizedDecoder = np.vectorize(decodeBytes)\n",
    "\n",
    "    training_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_train.arff\"))\n",
    "    train_x = training_data[:,:-1]\n",
    "    train_y = vectorizedDecoder(training_data[:,-1])\n",
    "\n",
    "    testing_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_test.arff\"))\n",
    "    test_x = testing_data[:,:-1]\n",
    "    test_y = vectorizedDecoder(testing_data[:,-1])\n",
    "\n",
    "    knn = KNNClassifier(weight_type=\"\")\n",
    "    knn.fit(train_x, train_y)\n",
    "    print(knn.score(test_x, test_y, [3]))\n",
    "\n",
    "    knn2 = KNNClassifier()\n",
    "    knn2.fit(train_x, train_y)\n",
    "    print(knn2.score(test_x, test_y, [3]))\n",
    "do_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vWiTdlbR2Xh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. KNN with and without normalization\n",
    "\n",
    "- Use the [magic telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope) task with this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff) \n",
    "\n",
    "### 2.1 (5%)\n",
    "- Try it with k=3 and without distance weighting and *without* normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4SSoasDQSKXb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8082808280828083]\n"
     ]
    }
   ],
   "source": [
    "def do_magic_without_normalization():\n",
    "    vectorizedDecoder = np.vectorize(decodeBytes)\n",
    "\n",
    "    training_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff\"))\n",
    "    train_x = training_data[:,:-1]\n",
    "    train_y = vectorizedDecoder(training_data[:,-1])\n",
    "\n",
    "    testing_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff\"))\n",
    "    test_x = testing_data[:,:-1]\n",
    "    test_y = vectorizedDecoder(testing_data[:,-1])\n",
    "\n",
    "    knn = KNNClassifier(weight_type=\"\")\n",
    "    knn.fit(train_x, train_y)\n",
    "    print(knn.score(test_x, test_y, [3]))\n",
    "do_magic_without_normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 (5%)\n",
    "- Try it with k=3 without distance weighting and *with* normalization (input features normalized between 0 and 1). Use the normalization formula (x-xmin)/(xmax-xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8304830483048304]\n"
     ]
    }
   ],
   "source": [
    "def do_magic_with_normalization():\n",
    "    vectorizedDecoder = np.vectorize(decodeBytes)\n",
    "\n",
    "    training_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff\"))\n",
    "    train_x = training_data[:,:-1]\n",
    "    train_y = vectorizedDecoder(training_data[:,-1])\n",
    "\n",
    "    testing_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff\"))\n",
    "    test_x = testing_data[:,:-1]\n",
    "    test_y = vectorizedDecoder(testing_data[:,-1])\n",
    "\n",
    "    combined_data = np.concatenate((train_x,test_x))\n",
    "    combined_normalized = normalize_data(combined_data)\n",
    "    num_train_x_rows, _ = train_x.shape\n",
    "\n",
    "    normalized_train_x = combined_normalized[:num_train_x_rows, :]\n",
    "    normalized_test_x = combined_normalized[num_train_x_rows:, :]\n",
    "\n",
    "    knn2 = KNNClassifier(weight_type=\"\")\n",
    "    knn2.fit(normalized_train_x, train_y)\n",
    "    print(knn2.score(normalized_test_x, test_y, [3]))\n",
    "do_magic_with_normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Discuss the results of using normalized data vs. unnormalized data*\n",
    "The model trained on the normalized data was a bit more accurate than the model trained on the unnormalized data. After predicting the test set with k=3, I got an accuracy of approximately 80% using the unnormalized data and an accuracy of 83% using the normalized data. This makes sense, as attributes that naturally have a larger range generally have higher distance metrics whereas if the data is normalized, each feature gets an opportunity to provide input as the distance metrics are now standardized. To normalize the data, I combined both the training and test datasets first so that the range across which they were normalized was equivalent. One thing that could be worth further investigation is the scale across which the data is normalized. For example if the data was not necessarily linear, it could be better to find another normalizing function that better distributed the data to give the model a better opportunity to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 (5%)\n",
    "\n",
    "- Using your normalized data, create one graph with classification accuracy on the test set on the y-axis and k values on the x-axis. \n",
    "    - Use odd values of k from 1 to 15.\n",
    "- As a rough sanity check, typical knn accuracies for the magic telescope data set are 75-85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def graph_magic():\n",
    "    vectorizedDecoder = np.vectorize(decodeBytes)\n",
    "\n",
    "    training_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff\"))\n",
    "    train_x = training_data[:,:-1]\n",
    "    train_y = vectorizedDecoder(training_data[:,-1])\n",
    "\n",
    "    testing_data = np.array(load_data(r\"https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff\"))\n",
    "    test_x = testing_data[:,:-1]\n",
    "    test_y = vectorizedDecoder(testing_data[:,-1])\n",
    "\n",
    "    combined_data = np.concatenate((train_x,test_x))\n",
    "    combined_normalized = normalize_data(combined_data)\n",
    "    num_train_x_rows, _ = train_x.shape\n",
    "\n",
    "    normalized_train_x = combined_normalized[:num_train_x_rows, :]\n",
    "    normalized_test_x = combined_normalized[num_train_x_rows:, :]\n",
    "\n",
    "    knn2 = KNNClassifier(weight_type=\"\")\n",
    "    knn2.fit(normalized_train_x, train_y)\n",
    "    k_array = [1,3,5,7,9,11,13,15]\n",
    "    accuracy = knn2.score(normalized_test_x, test_y, [1,3,5,7,9,11,13,15])\n",
    "    print(accuracy)\n",
    "\n",
    "    plt.plot(k_array, accuracy)\n",
    "    plt.title(\"Classification Accuracy by k-value\")\n",
    "    plt.xlabel(\"K-value\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "graph_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When I ran knn on the normalized data and graphed it for different k-values, I was somewhat surprised at how quickly the accuracy leveled off as k-values increased, flattening out at around k=5. This makes sense, because with k-values smaller than 5, noise can have a much larger impact on the data. If you happened to predict a point close to a couple of noisy points, you would predict the wrong class. However, with k=5, it would be much less likely that you would find 5 noisy points in the same region. I continued to increase the k-values (although not pictured) the classification accuracy would go down. This also makes sense, because eventually you would be looking at neighbors in different \"clusters\" or with very different attributes, and they would get a chance to vote. This would eventually just classify everything as the majority if you weren't using a weighted distance metric."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*For the rest of the experiments use only normalized data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. (10%) KNN regression\n",
    "\n",
    "- Use the regression variation of your algorithm (without distance weighting) on the [housing price prediction](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) problem.  Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_test.arff). Note this data set has an example of an inappropriate use of data which we will discuss.\n",
    "- Use Mean Square Error (MSE) on the test set as your accuracy metric for this case\n",
    "    - Do not normalize regression output values\n",
    "- Graph MSE on the test set with odd values of k from 1 to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KBGUn43ASiXW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def do_housing():\n",
    "    normalized_train_x, train_y, normalized_test_x, test_y = get_normalized_housing_data()\n",
    "    knn = KNNClassifier(weight_type='',regression=True)\n",
    "    knn.fit(normalized_train_x, train_y)\n",
    "    k_array = [1,3,5,7,9,11,13,15]\n",
    "    mses = knn.score(normalized_test_x, test_y, k_array)\n",
    "    print(mses)\n",
    "\n",
    "    plt.plot(k_array, mses)\n",
    "    plt.title(\"Housing MSE by k-value\")\n",
    "    plt.xlabel(\"K-value\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.show()\n",
    "do_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When examining the results of the MSE graph, I found it somewhat interesting that the knn model with the lowest MSE used k=5. This follows a similar pattern to the classification accuracy of the telescope dataset as the average/choice of a few closer points gives a better score than only a few possibly noisy points or a set of points so large that doesn't accurately capture the information within a certain feature region. I also noticed the MSE seemed to level out at about k=13. I imagine that if we used a weighted distance metric, the resulting graph would not be as jagged, but have somewhat more similar MSE values as the k-values increased, although the MSE might slightly get worse as the k-value increases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v19fpixqTe-7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. KNN with distance weighting\n",
    "- Repeat your experiments for magic telescope and housing using distance-weighted (inverse of distance squared) voting and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1 (7.5%) Magic Telescope Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZCPFUAGTS2sX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def do_magic_distance_weighting():\n",
    "    normalized_train_x, train_y, normalized_test_x, test_y = get_normalized_telescope_data()\n",
    "    knn = KNNClassifier()\n",
    "    knn.fit(normalized_train_x, train_y)\n",
    "    k_array = [1,3,5,7,9,11,13,15]\n",
    "    accuracy = knn.score(normalized_test_x, test_y, k_array)\n",
    "    print(accuracy)\n",
    "\n",
    "    plt.plot(k_array, accuracy)\n",
    "    plt.title(\"Classification Accuracy by k-value (with weighted metric)\")\n",
    "    plt.xlabel(\"K-value\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "do_magic_distance_weighting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2 (7.5%) Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def do_housing_distance_weighting():\n",
    "    normalized_train_x, train_y, normalized_test_x, test_y = get_normalized_housing_data()\n",
    "    knn = KNNClassifier(regression=True)\n",
    "    knn.fit(normalized_train_x, train_y)\n",
    "    k_array = [1,3,5,7,9,11,13,15]\n",
    "    mses = knn.score(normalized_test_x, test_y, k_array)\n",
    "    print(mses)\n",
    "\n",
    "    plt.plot(k_array, mses)\n",
    "    plt.title(\"Housing MSE by k-value (with weighted metric)\")\n",
    "    plt.xlabel(\"K-value\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.show()\n",
    "do_housing_distance_weighting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Discuss your results*\n",
    "Using distance weighting slightly improved the accuracy of the Magic Telescope Dataset classification and the MSE of the housing dataset. The Telescope classification accuracy was very similar and followed a similar shape to that of classification without the weighted inverse square distance metric. The classification accuracy was also less volatile when higher k-values were reached. This makes sense as points get farther and farther away from the original point they have less of a weight and can't sway the selection (vote) very much whereas without the weighted distance metric each point had an exactly equal impact on the selection, so points located farther away could determine the weight of a single point simply by majority no matter what its feature values were if the k-value was high enough.\n",
    "\n",
    "I thought that it was fascinating when I saw the graph of the Housing dataset MSE by k-value because it followed the prediction I made earlier. The MSE stabilized instead of wildly oscillating. When the k-value increases and more points are used that are farther away, they don't have such a heavy impact on the MSE as they aren't weighted very much. Eventually we would expect the MSE to get worse as we start including too many points in the classification that are dissimilar from the original point. At around k=7 the best MSE was reached which was a larger value than the optimal for the non-distance-weighted model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. (10%) KNN with nominal and unknown data\n",
    "\n",
    "- Use the [credit-approval](https://archive.ics.uci.edu/ml/datasets/Credit+Approval) task and this [dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/credit_approval.arff)\n",
    "    - Use a 70/30 split of the data for the training/test set\n",
    "- Note that this set has both continuous and nominal attributes, together with don’t know values. \n",
    "- Implement and justify a distance metric which supports continuous, nominal, and don’t know attribute values\n",
    "    - You need to handle don't knows with the distance metric, not by imputing a value.\n",
    "    - More information on distance metrics can be found [here](https://www.jair.org/index.php/jair/article/view/10182/24168).\n",
    "- Use your own choice for k.\n",
    "- As a rough sanity check, typical knn accuracies for the credit data set are 70-80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8454106280193237, 0.8599033816425121, 0.8599033816425121]\n"
     ]
    }
   ],
   "source": [
    "def do_credit():\n",
    "    data = np.array(pd.read_csv(r\"https://raw.githubusercontent.com/maclaurin36/CS312-knn/master/crx.csv\"))\n",
    "    nominal_feature_indices = [0,3,4,5,6,8,9,11,12]\n",
    "    continuous_feature_indices = [1,2,7,10,13,14]\n",
    "    coltypes = ['nominal','continuous','continuous','nominal','nominal','nominal','nominal','continuous','nominal','nominal','continuous','nominal','nominal','continuous','continuous']\n",
    "    for colIndex in continuous_feature_indices:\n",
    "        normalize_col = data[:, colIndex]\n",
    "        no_missing = normalize_col[normalize_col!='?'].astype(float)\n",
    "        max = np.max(no_missing)\n",
    "        min = np.min(no_missing)\n",
    "        for i, val in enumerate(normalize_col):\n",
    "            if val == '?':\n",
    "                data[i,colIndex] = '?'\n",
    "            else:\n",
    "                data[i,colIndex] = (float(normalize_col[i]) - min) / float(max - min)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data[:,:-1], data[:,-1], test_size=0.3)\n",
    "    knn = KNNClassifier(hasNominal=True, columntype=coltypes)\n",
    "    knn.fit(train_x, train_y)\n",
    "    print(knn.score(test_x, test_y, [3,5,7]))\n",
    "do_credit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Explain and justify your distance metric and discuss your results*\n",
    "My distance metric was that the normalized distance between attributes was 1 if the attribute was unknown. This led the model to consider those data points as farther away. This is useful as there may be an underlying reason that the data was not collected or known. This underlying reason could lead to a different neighborhood of points, so it is likely best to just ignore the missing attributes.\n",
    "For the nominal attributes, I used a simple distance metric of 1 if they didn't match and 0 if they did match. I did this because without more in-depth knowledge of the categories I couldn't determine if some features were generally more closely related or not. This metric was also much easier to compute than comparing the output classes of the training data for a given class and seeing which ones were most similar. I did however consider implementing that model and think it could be a valuable approach to handling unknown values in the credit data set.\n",
    "For the continuous attributes I simply used the euclidean distance as was used in the previous exercises with unknowns handled as simply a distance of 1 from any point.\n",
    "\n",
    "After testing knn models with k= 3, 4, and 5 on the credit dataset with the above piecewise distance metric, I generally got an average accuracy in the lower to mid 80s, varying according to the training test split with occasional results in the 70s. Given these results, I believe that the distance metric I used was a good way to classify this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. (10%) Scikit-Learn KNN \n",
    "- Use the scikit-learn KNN version on magic telescope and housing and compare your results\n",
    "- Try out different hyperparameters to see how well you can do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OFQv70W2VyqJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification with telescope\n",
      "k=5\n",
      "0.8432343234323433\n",
      "k=10\n",
      "0.8402340234023402\n",
      "k=5, distance weightings\n",
      "0.8460846084608461\n",
      "k=10, distance weightings\n",
      "0.8492349234923492\n",
      "\n",
      "Regression with Housing data\n",
      "k=5\n",
      "0.7957406361552128\n",
      "k=10\n",
      "0.7081491553203445\n",
      "k=5, distance weightings\n",
      "0.8356040739475871\n",
      "k=10, distance weightings\n",
      "0.8441169760654081\n"
     ]
    }
   ],
   "source": [
    "def run_scikit():\n",
    "    normalized_train_x, train_y, normalized_test_x, test_y = get_normalized_telescope_data()\n",
    "\n",
    "    print(\"Classification with telescope\")\n",
    "    knnClassifier = KNeighborsClassifier(5)\n",
    "    knnClassifier.fit(normalized_train_x, train_y)\n",
    "    print(\"k=5\")\n",
    "    print(knnClassifier.score(normalized_test_x, test_y))\n",
    "\n",
    "    knnClassifier = KNeighborsClassifier(n_neighbors=10)\n",
    "    knnClassifier.fit(normalized_train_x, train_y)\n",
    "    print(\"k=10\")\n",
    "    print(knnClassifier.score(normalized_test_x, test_y))\n",
    "\n",
    "    knnClassifier = KNeighborsClassifier(weights='distance')\n",
    "    knnClassifier.fit(normalized_train_x, train_y)\n",
    "    print(\"k=5, distance weightings\")\n",
    "    print(knnClassifier.score(normalized_test_x, test_y))\n",
    "\n",
    "    knnClassifier = KNeighborsClassifier(weights='distance', n_neighbors=10)\n",
    "    knnClassifier.fit(normalized_train_x, train_y)\n",
    "    print(\"k=10, distance weightings\")\n",
    "    print(knnClassifier.score(normalized_test_x, test_y))\n",
    "\n",
    "    print()\n",
    "    print(\"Regression with Housing data\")\n",
    "    normalized_train_x, train_y, normalized_test_x, test_y = get_normalized_housing_data()\n",
    "    knnRegressor = KNeighborsRegressor()\n",
    "    knnRegressor.fit(normalized_train_x, train_y)\n",
    "    print(\"k=5\")\n",
    "    print(knnRegressor.score(normalized_test_x, test_y))\n",
    "\n",
    "    knnRegressor = KNeighborsRegressor(n_neighbors=10)\n",
    "    knnRegressor.fit(normalized_train_x, train_y)\n",
    "    print(\"k=10\")\n",
    "    print(knnRegressor.score(normalized_test_x, test_y))\n",
    "\n",
    "    knnRegressor = KNeighborsRegressor(n_neighbors=7, weights='distance')\n",
    "    knnRegressor.fit(normalized_train_x, train_y)\n",
    "    print(\"k=7, distance weightings\")\n",
    "    print(knnRegressor.score(normalized_test_x, test_y))\n",
    "\n",
    "    knnRegressor = KNeighborsRegressor(weights='distance')\n",
    "    knnRegressor.fit(normalized_train_x, train_y)\n",
    "    print(\"k=10, distance weightings\")\n",
    "    print(knnRegressor.score(normalized_test_x, test_y))\n",
    "run_scikit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Report your comparison*\n",
    "Before I actually report the comparison of how well my models did when compared to scikit learns, I want to note that it amazes me how INCREDIBLY fast scikit learns models run in comparison to mine. I tried to use numpy operations to implement my model, but even so, the magic telescope dataset takes about 3ish minutes to run. Scikit learn's models take only a few seconds.\n",
    "\n",
    "When comparing the results of the scikit learns models to my own for the telescope dataset, I found very little differences between the classification accuracies. Each model scored roughly between 84-85%. I tried adjusting the k-values and the weight metrics, and found that similar to my own model using inverse distance squared weightings and a k value between 5-10 yielded the highest accuracies.\n",
    "\n",
    "When comparing the results of the housing models with my own, I found that generally k-values in the lower teens did best just as well in scikit as they did in my own implementation. If we look at the graph I generated when running my model, we see that the minimum MSE was achieved at about k = 7. This is almost equivalent to scikit learn's results. There was also a decent amount of variability in the scoring of the regressor with the housing dataset with different parameter values, although this difference was removed/minimized when distance weightings were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlK-kijk8Mg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. (optional 5% extra credit): Reducing the data set\n",
    "- Choose either of the data sets above and use the best k value you found.\n",
    "- Implement a reduction algorithm that removes data points in some rational way such that performance does not drop too drastically on the test set given the reduced training set.\n",
    "- Compare your performance on the test set for the reduced and non-reduced versions and give the number (and percentage) of training examples removed from the original training set. \n",
    "    - Note that performance for magic telescope is classification accuracy and for housing it is mean squared error.\n",
    "    - Magic Telescope has about 12,000 instances and if you use a leave one out style of testing for your data set reduction, then your algorithm will run slow since that is n^2 at each step.\n",
    "        - If you wish, you may use a random subset of 2,000 of the magic telescope instances.\n",
    "    - More information on reduction techniques can be found [here](http://axon.cs.byu.edu/~martinez/classes/478/slides/IBL.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Discussion. How well did it do?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab 1 - perceptron",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}